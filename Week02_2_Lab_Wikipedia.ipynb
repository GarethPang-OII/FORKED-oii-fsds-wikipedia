{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science \n",
    "## Week 2 Day 2 Lab: Downloading to Wikipedia\n",
    "\n",
    "Today we will review some changes to the Wikipedia code. These changes will considerably alter what you are able to do with this code. The end result will be a set of two folders, `data` and `dataframes` which you can use for analysis of Wikipedia. \n",
    "\n",
    "The code has now been altered on my end in several ways: \n",
    "- use and report curl from special export to get a complete history of a page. \n",
    "- considerably expanded reporting and commenting.\n",
    "- new arguments available to the script include --count_only \n",
    "\n",
    "There is also now a second script available `xml_to_dataframe.py` which can be used to then process these files and turn them into separate DataFrames. These DataFrames are stored as .feather files and can be loaded with the code below. \n",
    "\n",
    "You should review the `xml_to_dataframe.py` file as all the operations within that file have been covered in class with the exception of TQDM but you can see how that works in practice. \n",
    "\n",
    "You will note that this version does not use recursion to count the files. Instead it more literally looks within year and month. This is sufficient for this work, but with a deeper folder structure and one where the structure is less certain this approach would not be robust. On the other hand, by assuming year and month it allows for some interesting statistics about the year and month to be displayed. In your own work you may now consider whether to approach a task with a more general but often more abstract solution or a more specific but often more fragile solution. You can see in Jon's solution that he used a clever way to simply count all the files using a global and letting the global handle the recursion (`download_and_count_revisions_solution.py`).\n",
    "\n",
    "You should now be able to download a complete history for a single wikipedia page and process that as a DataFrame. Confirm that you can do this with the code yourself. Then discuss among your group:\n",
    "1. Which two (or more) public figures are worth comparing and why. \n",
    "2. Prior to any specific time series analysis, consider your expectations for this exploratory comparison.  \n",
    "\n",
    "Draw upon your group's potential expertise in social science to come up with a theoretically informed rationale for a given comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in Changes to a Repository \n",
    "\n",
    "First you will want to merge files from an upstream branch (mine). These instructions will show how to do that from the terminal. You will want to be in the oii-fsds-wikipedia folder when entering these commands. Note especially **Step 3**. If you do this it will overwrite `download_wiki_revisions.py` so consider making a backup. \n",
    "\n",
    "1. **Add the original repository as a remote:**\n",
    "   ```sh\n",
    "   git remote add upstream https://github.com/berniehogan/oii-fsds-wikipedia.git\n",
    "   ```\n",
    "\n",
    "2. **Fetch the changes from the original repository:**\n",
    "   ```sh\n",
    "   git fetch upstream\n",
    "   ```\n",
    "\n",
    "3. **Backup any local changes:**\n",
    "   If you have your own versions of files like `download_wiki_revisions.py`, you should rename the file first to avoid conflicts:\n",
    "   ```sh\n",
    "   mv download_wiki_revisions.py download_wiki_revisions_backup.py\n",
    "   ```\n",
    "\n",
    "4. **Merge upstream changes into your local main branch:**\n",
    "   ```sh\n",
    "   git merge upstream/main\n",
    "   ```\n",
    "\n",
    "5. **Resolve any conflicts and commit the changes:**\n",
    "   You should resolve any conflicts that arise during the merge and then commit the changes:\n",
    "   ```sh\n",
    "   git add .\n",
    "   git commit -m \"Merge changes from upstream\"\n",
    "   ```\n",
    "\n",
    "6. **Push the changes to your GitHub repository:**\n",
    "   ```sh\n",
    "   git push origin main\n",
    "   ```\n",
    "\n",
    "7. **Test your code after merging:**\n",
    "   You should test your code to ensure everything works correctly after the integration.\n",
    "\n",
    "By following these steps, you should be able to integrate the latest changes from my repository while preserving your own custom modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, you can use the script below if you wish in order to run the commands directly within a Jupyter notebook rather than via that terminal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important note for writing commands for terminal in python notebook\n",
    "\n",
    "**Use `os.system(<terminal code>)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: bs4 in ./.venv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: pathlib in ./.venv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: datetime in ./.venv/lib/python3.12/site-packages (5.5)\n",
      "Requirement already satisfied: zope.interface in ./.venv/lib/python3.12/site-packages (from datetime) (7.1.0)\n",
      "Requirement already satisfied: pytz in ./.venv/lib/python3.12/site-packages (from datetime) (2024.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from zope.interface->datetime) (75.2.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.66.5)\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.12/site-packages (5.3.0)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./.venv/lib/python3.12/site-packages (from pyarrow) (2.1.2)\n",
      "Using cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-17.0.0\n"
     ]
    }
   ],
   "source": [
    "# Import pre-requisite packages\n",
    "\n",
    "os.system('pip install pandas && pip freeze > requirements.txt')\n",
    "os.system('pip install requests && pip freeze > requirements.txt')\n",
    "os.system('pip install bs4 && pip freeze > requirements.txt')\n",
    "os.system('pip install pathlib && pip freeze > requirements.txt')\n",
    "os.system('pip install datetime && pip freeze > requirements.txt')\n",
    "os.system('pip install tqdm && pip freeze > requirements.txt')\n",
    "os.system('pip install argparse && pip freeze > requirements.txt')\n",
    "os.system('pip install lxml && pip freeze > requirements.txt')\n",
    "os.system('pip install pyarrow && pip freeze > requirements.txt')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define articles we want to download\n",
    "article1 = \"Data_science\"\n",
    "article2 = \"Machine_learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories if they don't exist (if it exists it won't overwrite or throw an error)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"DataFrames\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading revisions for first article...\n",
      "Downloading complete history of Data_science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading revisions: 35.3MiB [00:01, 24.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1709 revisions. Organizing into directory structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1709/1709 [00:02<00:00, 821.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final revision counts:\n",
      "Found 1709 total revisions for 'Data_science'.\n",
      "\n",
      "Breakdown by year:\n",
      "  2012: 91 revisions\n",
      "  2013: 127 revisions\n",
      "  2014: 73 revisions\n",
      "  2015: 143 revisions\n",
      "  2016: 103 revisions\n",
      "  2017: 135 revisions\n",
      "  2018: 190 revisions\n",
      "  2019: 130 revisions\n",
      "  2020: 168 revisions\n",
      "  2021: 133 revisions\n",
      "  2022: 185 revisions\n",
      "  2023: 110 revisions\n",
      "  2024: 121 revisions\n",
      "\n",
      "Downloading revisions for second article...\n",
      "Downloading complete history of Machine_learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading revisions: 238MiB [00:20, 11.4MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3887 revisions. Organizing into directory structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3887/3887 [00:08<00:00, 464.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final revision counts:\n",
      "Found 3887 total revisions for 'Machine_learning'.\n",
      "\n",
      "Breakdown by year:\n",
      "  2003: 6 revisions\n",
      "  2004: 33 revisions\n",
      "  2005: 103 revisions\n",
      "  2006: 138 revisions\n",
      "  2007: 130 revisions\n",
      "  2008: 71 revisions\n",
      "  2009: 74 revisions\n",
      "  2010: 132 revisions\n",
      "  2011: 129 revisions\n",
      "  2012: 113 revisions\n",
      "  2013: 96 revisions\n",
      "  2014: 152 revisions\n",
      "  2015: 219 revisions\n",
      "  2016: 261 revisions\n",
      "  2017: 263 revisions\n",
      "  2018: 270 revisions\n",
      "  2019: 293 revisions\n",
      "  2020: 297 revisions\n",
      "  2021: 244 revisions\n",
      "  2022: 298 revisions\n",
      "  2023: 328 revisions\n",
      "  2024: 237 revisions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download revisions for both articles\n",
    "# Using `os.system(<code>)` allows us to execute code as if in terminal\n",
    "# Note that download_wiki_revisions.py already has the import packages functions \n",
    "    # We just need to make sure that they are in the terminal\n",
    "\n",
    "print(\"Downloading revisions for first article...\")\n",
    "os.system(f'python download_wiki_revisions.py \"{article1}\"') \n",
    "print(\"\\nDownloading revisions for second article...\")\n",
    "os.system(f'python download_wiki_revisions.py \"{article2}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting revisions to DataFrames...\n",
      "Processing with text length only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Cat: 100%|██████████| 1/1 [00:00<00:00, 51.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Cat:\n",
      "Total revisions: 10\n",
      "Date range: 2024-10-14 05:29:09+00:00 to 2024-10-16 14:43:33+00:00\n",
      "Unique contributors: 3\n",
      "Average text length: 164003.1 characters\n",
      "\n",
      "Summary for Climate Change:\n",
      "Total revisions: 18\n",
      "Date range: 2002-02-25 15:51:15+00:00 to 2024-01-04 18:20:42+00:00\n",
      "Unique contributors: 14\n",
      "Average text length: 293.2 characters\n",
      "\n",
      "Summary for Dog:\n",
      "Total revisions: 10\n",
      "Date range: 2024-10-19 20:54:43+00:00 to 2024-10-21 12:37:43+00:00\n",
      "Unique contributors: 8\n",
      "Average text length: 190122.7 characters\n",
      "\n",
      "Summary for Leopard:\n",
      "Total revisions: 50\n",
      "Date range: 2024-07-20 22:40:23+00:00 to 2024-10-19 01:05:05+00:00\n",
      "Unique contributors: 13\n",
      "Average text length: 109459.4 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Climate Change: 100%|██████████| 1/1 [00:00<00:00, 109.46batch/s]\n",
      "Processing Dog: 100%|██████████| 1/1 [00:00<00:00, 39.71batch/s]\n",
      "Processing Leopard: 100%|██████████| 1/1 [00:00<00:00, 11.48batch/s]\n",
      "Processing Machine_learning: 100%|██████████| 4/4 [00:02<00:00,  1.64batch/s]\n",
      "Processing Hamster:   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Machine_learning:\n",
      "Total revisions: 3887\n",
      "Date range: 2003-05-25 06:03:17+00:00 to 2024-10-21 15:03:51+00:00\n",
      "Unique contributors: 1098\n",
      "Average text length: 59622.2 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Hamster: 100%|██████████| 1/1 [00:00<00:00,  1.71batch/s]\n",
      "Processing Data_science:   0%|          | 0/2 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Hamster:\n",
      "Total revisions: 1000\n",
      "Date range: 2011-02-04 18:00:15+00:00 to 2024-08-12 13:16:07+00:00\n",
      "Unique contributors: 343\n",
      "Average text length: 24421.3 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data_science: 100%|██████████| 2/2 [00:00<00:00,  2.38batch/s]\n",
      "Processing Tiger:   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Data_science:\n",
      "Total revisions: 1709\n",
      "Date range: 2012-04-11 17:34:10+00:00 to 2024-09-04 22:32:11+00:00\n",
      "Unique contributors: 466\n",
      "Average text length: 19660.1 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Tiger: 100%|██████████| 1/1 [00:01<00:00,  1.74s/batch]\n",
      "Processing Cheetah: 100%|██████████| 1/1 [00:00<00:00,  9.89batch/s]\n",
      "Processing Singapore:   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Tiger:\n",
      "Total revisions: 1000\n",
      "Date range: 2024-03-16 17:22:00+00:00 to 2024-10-21 10:33:06+00:00\n",
      "Unique contributors: 51\n",
      "Average text length: 146214.8 characters\n",
      "\n",
      "Summary for Cheetah:\n",
      "Total revisions: 50\n",
      "Date range: 2024-05-19 15:12:28+00:00 to 2024-10-07 03:39:46+00:00\n",
      "Unique contributors: 14\n",
      "Average text length: 190603.3 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Singapore: 100%|██████████| 1/1 [00:03<00:00,  3.06s/batch]\n",
      "Processing Kangaroo: 100%|██████████| 1/1 [00:00<00:00, 18.10batch/s]\n",
      "Processing New Zealand:   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Singapore:\n",
      "Total revisions: 1000\n",
      "Date range: 2021-08-08 10:07:14+00:00 to 2024-10-21 11:34:25+00:00\n",
      "Unique contributors: 352\n",
      "Average text length: 286322.4 characters\n",
      "\n",
      "Summary for Kangaroo:\n",
      "Total revisions: 50\n",
      "Date range: 2022-10-06 11:48:28+00:00 to 2024-09-26 12:40:57+00:00\n",
      "Unique contributors: 36\n",
      "Average text length: 69492.4 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing New Zealand: 100%|██████████| 1/1 [00:00<00:00,  6.05batch/s]\n",
      "Processing Hedgehog: 100%|██████████| 1/1 [00:00<00:00, 27.75batch/s]\n",
      "Processing Cow: 100%|██████████| 1/1 [00:00<00:00, 33.17batch/s]\n",
      "Processing Lion: 100%|██████████| 1/1 [00:00<00:00, 10.95batch/s]\n",
      "Processing Elephant:   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for New Zealand:\n",
      "Total revisions: 50\n",
      "Date range: 2024-09-05 22:08:40+00:00 to 2024-10-19 05:06:02+00:00\n",
      "Unique contributors: 24\n",
      "Average text length: 272857.7 characters\n",
      "\n",
      "Summary for Hedgehog:\n",
      "Total revisions: 50\n",
      "Date range: 2023-02-24 01:48:59+00:00 to 2024-10-18 23:58:46+00:00\n",
      "Unique contributors: 28\n",
      "Average text length: 32363.1 characters\n",
      "\n",
      "Summary for Cow:\n",
      "Total revisions: 87\n",
      "Date range: 2001-05-17 08:19:29+00:00 to 2023-02-01 15:02:19+00:00\n",
      "Unique contributors: 41\n",
      "Average text length: 78.2 characters\n",
      "\n",
      "Summary for Lion:\n",
      "Total revisions: 50\n",
      "Date range: 2024-08-19 19:25:04+00:00 to 2024-10-18 14:43:17+00:00\n",
      "Unique contributors: 21\n",
      "Average text length: 147368.1 characters\n",
      "\n",
      "Summary for Elephant:\n",
      "Total revisions: 50\n",
      "Date range: 2024-05-15 16:14:19+00:00 to 2024-10-13 01:25:45+00:00\n",
      "Unique contributors: 31\n",
      "Average text length: 128240.7 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Elephant: 100%|██████████| 1/1 [00:00<00:00, 12.28batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all downloaded revisions to DataFrames\n",
    "print(\"\\nConverting revisions to DataFrames...\")\n",
    "os.system('python xml_to_dataframe.py --data-dir ./data --output-dir ./DataFrames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying DataFrame contents...\n"
     ]
    }
   ],
   "source": [
    "# Load and verify one of the DataFrames\n",
    "print(\"\\nVerifying DataFrame contents...\")\n",
    "df = pd.read_feather(f\"DataFrames/{article1}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1709 entries, 479 to 759\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   revision_id  1709 non-null   object             \n",
      " 1   timestamp    1709 non-null   datetime64[ns, UTC]\n",
      " 2   username     1123 non-null   object             \n",
      " 3   userid       1123 non-null   object             \n",
      " 4   comment      1372 non-null   object             \n",
      " 5   text_length  1709 non-null   int64              \n",
      " 6   year         1709 non-null   object             \n",
      " 7   month        1709 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(6)\n",
      "memory usage: 120.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revision_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>userid</th>\n",
       "      <th>comment</th>\n",
       "      <th>text_length</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>1244076203</td>\n",
       "      <td>2024-09-04 22:32:11+00:00</td>\n",
       "      <td>Arachnidly</td>\n",
       "      <td>47739713</td>\n",
       "      <td>removed cuz scientific method linked in intro ...</td>\n",
       "      <td>28671</td>\n",
       "      <td>2024</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1244075613</td>\n",
       "      <td>2024-09-04 22:27:46+00:00</td>\n",
       "      <td>Arachnidly</td>\n",
       "      <td>47739713</td>\n",
       "      <td>removed unrelated journal \"scientific data\" pa...</td>\n",
       "      <td>28695</td>\n",
       "      <td>2024</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1243594551</td>\n",
       "      <td>2024-09-02 10:24:08+00:00</td>\n",
       "      <td>Michaelmalak</td>\n",
       "      <td>14994222</td>\n",
       "      <td>Undid revision [[Special:Diff/1243592758|12435...</td>\n",
       "      <td>28719</td>\n",
       "      <td>2024</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>1243592758</td>\n",
       "      <td>2024-09-02 10:03:39+00:00</td>\n",
       "      <td>Iniyavalsha333</td>\n",
       "      <td>48376643</td>\n",
       "      <td>I added new info about data science.</td>\n",
       "      <td>29437</td>\n",
       "      <td>2024</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1243591540</td>\n",
       "      <td>2024-09-02 09:49:07+00:00</td>\n",
       "      <td>Michaelmalak</td>\n",
       "      <td>14994222</td>\n",
       "      <td>Undid revision [[Special:Diff/1243589808|12435...</td>\n",
       "      <td>28719</td>\n",
       "      <td>2024</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    revision_id                 timestamp        username    userid  \\\n",
       "479  1244076203 2024-09-04 22:32:11+00:00      Arachnidly  47739713   \n",
       "475  1244075613 2024-09-04 22:27:46+00:00      Arachnidly  47739713   \n",
       "474  1243594551 2024-09-02 10:24:08+00:00    Michaelmalak  14994222   \n",
       "477  1243592758 2024-09-02 10:03:39+00:00  Iniyavalsha333  48376643   \n",
       "478  1243591540 2024-09-02 09:49:07+00:00    Michaelmalak  14994222   \n",
       "\n",
       "                                               comment  text_length  year  \\\n",
       "479  removed cuz scientific method linked in intro ...        28671  2024   \n",
       "475  removed unrelated journal \"scientific data\" pa...        28695  2024   \n",
       "474  Undid revision [[Special:Diff/1243592758|12435...        28719  2024   \n",
       "477               I added new info about data science.        29437  2024   \n",
       "478  Undid revision [[Special:Diff/1243589808|12435...        28719  2024   \n",
       "\n",
       "    month  \n",
       "479    09  \n",
       "475    09  \n",
       "474    09  \n",
       "477    09  \n",
       "478    09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "display(df.info())\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic statistics:\n",
      "Total number of revisions: 1709\n",
      "Date range: from 2012-04-11 17:34:10+00:00 to 2024-09-04 22:32:11+00:00\n",
      "Number of unique editors: 466\n"
     ]
    }
   ],
   "source": [
    "# Display some basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"Total number of revisions: {len(df)}\")\n",
    "print(f\"Date range: from {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Number of unique editors: {df['username'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
